{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff2b311",
   "metadata": {},
   "source": [
    "## Data download task for SOC-D\n",
    "\n",
    "Ever found yourself having to download climate / chemical deposition data and put it in an excel spreadsheet? Well, some people like excel, so in the interests of recording how it was done, the following notebook has been created. \n",
    "\n",
    "The data in question is MET office climate vars and deposition data from CEH's very own data repository. (*It is worth noting that the MET office data could be processed on JASMIN, so you could eliminate the downloading aspect of this.*)\n",
    "\n",
    "The request here was to download the MET and chemical deposition data, attribute a CSS points file with the data, then write this data to an excel spreadsheet where each sheet was seperate variable.\n",
    "\n",
    "For the underlying processes see [here](https://github.com/Ciaran1981/eot/blob/bdd6e4a75fecd38620af0b8b3cf480a4f94fdcc1/eot/met_tseries.py#L161) (writing the vars to geomnetry), [here ](https://github.com/Ciaran1981/eot/blob/bdd6e4a75fecd38620af0b8b3cf480a4f94fdcc1/eot/met_tseries.py#L241) (writing the MET vars to a sheet) and [here](https://github.com/Ciaran1981/eot/blob/bdd6e4a75fecd38620af0b8b3cf480a4f94fdcc1/eot/met_tseries.py#L306) (writing the deposition vars to a sheet).\n",
    "\n",
    "This is dependent on a lib of functions I have made which can be found [here](https://github.com/Ciaran1981/eot). Follow the instructions to install. The dependencies used below are all covered by this lib. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94638634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from eot.downloader import dloadbatch, setup_sesh\n",
    "import eot.met_tseries as mt\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from joblib import Parallel, delayed\n",
    "from cdo import Cdo\n",
    "from pyexcelerate import Workbook\n",
    "from joblib import Parallel, delayed\n",
    "cdo = Cdo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed18d9",
   "metadata": {},
   "source": [
    "### Part 1: MET office data\n",
    "\n",
    "First, we must setup the session with NEODC to download the data from their server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897af9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"NEODC username\"\n",
    "passw = \"NEODC password\"\n",
    "\n",
    "setup_sesh(usr, pss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef2b42",
   "metadata": {},
   "source": [
    "Next an input shapefile - you will need to obtain your own - in this instance it was countryside survey points.\n",
    "This is for later when we attribute each point with both Met climate and CEH deposition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e173b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inShp = 'path/to/my/cssfile.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828ca18",
   "metadata": {},
   "source": [
    "We will use this string as a template url from which to create a list of download urls. These get updated, so check on the NEODC for the latest and adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac395fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_url16 = ('https://dap.ceda.ac.uk/badc/ukmo-hadobs/data/'\n",
    "              'insitu/MOHC/HadOBS/HadUK-Grid/v1.1.0.0/1km/hurs/'\n",
    "              'mon/latest/hurs_hadukgrid_uk_1km_mon_196101-196112.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b235e71f",
   "metadata": {},
   "source": [
    "For the MET vars the time period of interest was 1978-2022.\n",
    "\n",
    "Some ugly formatting for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd835d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs = np.arange(1978, 2022, 1)\n",
    "\n",
    "years = [str(y)+'01-'+str(y)+'12' for y in yrs]\n",
    "\n",
    "yrlist = [rain_url16.replace('196101-196112', y) for y in years]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab444d97",
   "metadata": {},
   "source": [
    "Here we make a directory for everything to go in along with subdirs for each var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_vars = ['tasmax', 'tasmin', 'tas', 'rainfall', 'sun', 'hurs', 'pv']\n",
    "\n",
    "os.mkdir('MetDwn78_21')\n",
    "\n",
    "os.chdir('MetDwn78_21')\n",
    "\n",
    "clmdirs = [os.mkdir(c) for c in clim_vars if not os.path.isdir(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e9d66",
   "metadata": {},
   "source": [
    "Now we make a set of input and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlin = []\n",
    "pthoot = []\n",
    "\n",
    "for c in clim_vars:\n",
    "    ins = [y.replace('hurs', c) for y in yrlist]\n",
    "    oots = [os.path.join(c, os.path.basename(i)) for i in ins]\n",
    "    urlin.append(ins)\n",
    "    pthoot.append(oots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58976647",
   "metadata": {},
   "source": [
    "Finally download the files.\n",
    "\n",
    "This is done in parallel where ```nt``` is the number of threads used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0cabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = len(clim_vars)\n",
    "\n",
    "_ = Parallel(n_jobs=nt, verbose=2)(delayed(dloadbatch)(u, c) for u, c in zip(urlin, clim_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a882f7",
   "metadata": {},
   "source": [
    "Now we can merge the individual netcdfs into single files for each var encompassing the whole period (1978-2021). We use the lib ```cdo``` for this purpose as it is fast. This was intialised in the first cell along with the module imports.\n",
    "\n",
    "```python\n",
    "cdo = Cdo()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "\n",
    "for p, n in tqdm(zip(pthoot, clim_vars)):\n",
    "    nm = n+'_78_21.nc'\n",
    "    ofile = cdo.cat(input=p, output=nm, options='-r')\n",
    "    final.append(ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17521a",
   "metadata": {},
   "source": [
    "Now we can write the data to an excel file. The 'excel workbook' is created using pyexcelerate, which after trying various solutions was the only reliable option.\n",
    "\n",
    "This is a 2 stage process:\n",
    "first extracting the time series data to a geo-dataframe,\n",
    "```python\n",
    "ndf = mt.met_time_series_to_sheet(f, inShp, n, espgout='epsg:4326')\n",
    "```\n",
    "Then writing this data in list form to the excel sheets.\n",
    "\n",
    "```python\n",
    "values = [ndf.columns] + list(ndf.values)\n",
    "ws = wb.new_sheet(n, data=values)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "\n",
    "for f, n in tqdm(zip(final, clim_vars)):\n",
    "    ndf = mt.met_time_series_to_sheet(f, inShp, n, espgout='epsg:4326')\n",
    "    values = [ndf.columns] + list(ndf.values)\n",
    "    ws = wb.new_sheet(n, data=values)\n",
    "\n",
    "wb.save(\"SOCD_MET.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf35c63",
   "metadata": {},
   "source": [
    "### Part 2: CEH chemical deposition data\n",
    "\n",
    "The nature of the CEH deposition data required the writing of additional functions and a more convoluted procedure due to it being stored rather oddly as a csv file, despite being in layout a grid.\n",
    "\n",
    "The coordinates of each data point in the 'grids' were also centroids, whereas opens source geospatial libs (e.g. GDAL) use the top left corner. Hence the 'grid points' had to be shifted prior to rasterisation.\n",
    "\n",
    "I am unaware of a way of accessing this data via command line/API from the CEH repository so the data would have to be downloaded manually.\n",
    "\n",
    "Please alter the directory as appropriate.\n",
    "\n",
    "I saved my deposition data according to the years it covered, hence the structure - below. You will have to do the same.\n",
    "\n",
    "**Part 2a: The 1986-2012 data**\n",
    "\n",
    "This [data](https://catalogue.ceh.ac.uk/documents/8e7644fe-9f17-4fc3-8e4e-8b10a42d5d50) has a different set of headings to the latter data (2012-19). The 86-12 headings are:\n",
    "```python\n",
    "['easting', 'northing', 'NOx', 'NHx', 'xSOx', 'CaMg', 'year']\n",
    "```\n",
    "**If you just want a unified version of both older and newer formats skip to section 2b.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "depdir = ('DepositionData')\n",
    "\n",
    "os.chdir(depdir)\n",
    "\n",
    "dirlist = ['1986-2012', '2013-15', '2017-19']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682114f",
   "metadata": {},
   "source": [
    "Here we get a list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9781d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob(os.path.join(depdir, keywrd))\n",
    "flist.sort()\n",
    "\n",
    "list86 = glob(os.path.join(dirlist[0], '*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03820fe",
   "metadata": {},
   "source": [
    "Fix the coordinates for later on. We have files for both forest and moorland assumed landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e812d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = pd.read_csv(list86[2])\n",
    "\n",
    "moor = pd.read_csv(list86[1])\n",
    "\n",
    "yrs  = list(forest.year.unique())\n",
    "\n",
    "# shift from centroid to top left\n",
    "forest['nx'] = forest['easting']-2500 #back (west)\n",
    "forest['ny'] = forest['northing']+2500 #up (north)\n",
    "\n",
    "moor['nx'] = moor['easting']-2500 #back (west)\n",
    "moor['ny'] = moor['northing']+2500 #up (north)\n",
    "\n",
    "# create new gdf with shifted coordinates\n",
    "fgdf = gpd.GeoDataFrame(forest, \n",
    "                       geometry=gpd.points_from_xy(forest.nx, forest.ny),\n",
    "\n",
    "mgdf = gpd.GeoDataFrame(moor, \n",
    "                       geometry=gpd.points_from_xy(moor.nx, moor.ny))\n",
    "\n",
    "fshape = list86[2][:-3]+'shp' # forest\n",
    "\n",
    "mshape = list86[1][:-3]+'shp' # moor\n",
    "\n",
    "fgdf.to_file(fshape)\n",
    "\n",
    "mgdf.to_file(mshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af04d4",
   "metadata": {},
   "source": [
    "We need the raster geo-transform to input into a later function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rgt\n",
    "_, frgt = mt.rasterize_point(fshape, \"year=1986\", 'H', outras=None,\n",
    "                     pixel_size=5000, dtype=6)\n",
    "\n",
    "_, mrgt = mt.rasterize_point(mshape, \"year=1986\", 'H', outras=None,\n",
    "                     pixel_size=5000, dtype=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f43c30",
   "metadata": {},
   "source": [
    "Create lists for the subsequent loops to create the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chems = fgdf.columns.tolist()[3:13]\n",
    "\n",
    "list8612f = []\n",
    "list8612m = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bcec7",
   "metadata": {},
   "source": [
    "Here we create a 3-D arrays of the chemical deposition files in year order for each landscape assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in tqdm(chems):\n",
    "    _,  farray = mt.create_chem_stk(fshape, c,  pixel_size=5000, dtype=6)\n",
    "    _,  marray = mt.create_chem_stk(mshape, c,  pixel_size=5000, dtype=6)\n",
    "    \n",
    "    list8612f.append(farray)\n",
    "    list8612m.append(marray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a530764",
   "metadata": {},
   "source": [
    "Lastly we can write the data to excel via sampling from the CSS points file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2425937",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(inShp) # recall the css file path at the start of the notebook\n",
    "\n",
    "wb = Workbook()\n",
    "\n",
    "# Forest\n",
    "for f, n in tqdm(zip(list8612f, chems)):\n",
    "    ndf = mt.met_time_series_to_sheet2(gdf, f, frgt, n, yrs)\n",
    "    values = [ndf.columns] + list(ndf.values)\n",
    "    ws = wb.new_sheet(n, data=values)\n",
    "\n",
    "wb.save(\"SOCD_forest_86-2012.xlsx\")\n",
    "\n",
    "# Moor\n",
    "for f, n in tqdm(zip(list8612m, chems)):\n",
    "    ndf = mt.met_time_series_to_sheet2(gdf, f, mrgt, n, yrs)\n",
    "    values = [ndf.columns] + list(ndf.values)\n",
    "    ws = wb.new_sheet(n, data=values)\n",
    "\n",
    "wb.save(\"SOCD_moor_86-2012.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45d201",
   "metadata": {},
   "source": [
    "**Section 2b:**\n",
    "\n",
    "Having done this for the original CEH deposition data (1986-12), we can move onto the later [datasets](https://catalogue.ceh.ac.uk/documents/fd8151e9-0ee2-4dfa-a254-470c9bb9bc1e), which, unfortunately have different headings/formatting. \n",
    "\n",
    "Pre 2012 format:\n",
    "```python\n",
    "['easting', 'northing', 'NOx', 'NHx', 'xSOx', 'CaMg', 'year']\n",
    "```\n",
    "\n",
    "Post 2012 format:\n",
    "```python\n",
    "['easting', 'northing', 'nox_f', 'nhx_f', 'nms_f', 'nm_ca_mg_f', 'year']\n",
    "```\n",
    "**The following loop merges old and new formats to produce 86-19 depostion files. If we are going to process the 2012-19 files, we are as well to unify the labels and provide a complete file.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['*/*forest*.csv', '*/*moor*.csv']\n",
    "outfs = ['ForestComplete.shp', 'MoorComplete.shp']\n",
    "\n",
    "for k, o in tqdm(zip(keywords, outfs)):\n",
    "\n",
    "    flist = glob(os.path.join(depdir, k))\n",
    "    flist.sort()\n",
    "\n",
    "    dflist = [pd.read_csv(f) for f in flist]\n",
    "\n",
    "    # First off select & change the columns in the big df to be same as later ones\n",
    "    # Old format\n",
    "    #['easting', 'northing', 'NOx', 'NHx', xSOx, 'CaMg', 'year']\n",
    "    # new format\n",
    "    #['easting', 'northing', 'nox_f', 'nhx_f', 'nms_f', 'nm_ca_mg_f', 'year']\n",
    "\n",
    "    dflist[1]['year']=2015\n",
    "    dflist[2]['year']=2017\n",
    "    dflist[3]['year']=2019\n",
    "\n",
    "    dflist[0] = dflist[0][['easting', 'northing', 'NOx', 'NHx', 'xSOx', 'CaMg', 'year']]\n",
    "    dflist[0].columns = dflist[2].columns\n",
    "\n",
    "    if k == '*/*forest*.csv':\n",
    "        #fix the column in no3 - columns (2019) has a format error in it ('nhx_f ')\n",
    "        dflist[3].columns = dflist[2].columns\n",
    "\n",
    "    new = pd.concat(dflist)\n",
    "\n",
    "    new['nx'] = new['easting']-2500 #back (west)\n",
    "    new['ny'] = new['northing']+2500 #up (north)\n",
    "\n",
    "    gd = gpd.GeoDataFrame(new, \n",
    "                geometry=gpd.points_from_xy(new.nx, new.ny))\n",
    "    \n",
    "    gd.set_crs(epsg=27700, inplace=True)\n",
    "    \n",
    "    gd.to_file(o)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f510b5",
   "metadata": {},
   "source": [
    "Read in the newly merged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgdf2 = gpd.read_file(outfs[0])\n",
    "mgdf2 = gpd.read_file(outfs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c439008",
   "metadata": {},
   "source": [
    "Get the information from each to pass into the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs  = list(fgdf.year.unique())\n",
    "\n",
    "chemsf = fgdf2.columns.tolist()[3:13]\n",
    "chemsm = mgdf2.columns.tolist()[3:13]\n",
    "\n",
    "# get the rgt\n",
    "_, rgtf = mt.rasterize_point(outfs[0], \"year=1986\", 'H', outras=None,\n",
    "                     pixel_size=5000, dtype=6)\n",
    "\n",
    "_, rgtm = mt.rasterize_point(outfs[1], \"year=1986\", 'H', outras=None,\n",
    "                     pixel_size=5000, dtype=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa6d30",
   "metadata": {},
   "source": [
    "Lastly we can write the data to excel via sampling from the CSS points file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list19presf = []\n",
    "list19presm = []\n",
    "\n",
    "\n",
    "for cf, cm in tqdm(chemsf, chemsm):\n",
    "    _,  farray = mt.create_chem_stk(outfs[0], cf,  pixel_size=5000, dtype=6)\n",
    "    _,  marray = mt.create_chem_stk(outfs[1], cm,  pixel_size=5000, dtype=6)\n",
    "    \n",
    "    list19presf.append(farray)\n",
    "    list19presm.append(marray)\n",
    "\n",
    "# Forest\n",
    "wb = Workbook()\n",
    "\n",
    "for f, n in tqdm(zip(list19presf, chemsf)):\n",
    "    ndf = mt.met_time_series_to_sheet2(gdf, f, rgt, n, yrs)\n",
    "    values = [ndf.columns] + list(ndf.values)\n",
    "    ws = wb.new_sheet(n, data=values)\n",
    "\n",
    "wb.save(\"SOCD_forest_86-2019.xlsx\")\n",
    "\n",
    "del wb\n",
    "\n",
    "# Moor\n",
    "wb = Workbook()\n",
    "\n",
    "for f, n in tqdm(zip(list19presm, chemsm)):\n",
    "    ndf = mt.met_time_series_to_sheet2(gdf, f, rgt, n, yrs)\n",
    "    values = [ndf.columns] + list(ndf.values)\n",
    "    ws = wb.new_sheet(n, data=values)\n",
    "\n",
    "wb.save(\"SOCD_moor_86-2019.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1807b2a",
   "metadata": {},
   "source": [
    "### README\n",
    "\n",
    "It is worth noting that the above does not provide a \"readme\" sheet. This could be added\n",
    "at the beginning of a workbook, though it is probably quicker to do this manually. See [here](https://github.com/kz26/PyExcelerate#styling-cells) for further details on formatting if required.\n",
    "\n",
    "The following will create a quick table of the old and new chemical formats and a hyperlink to the CEH repository in cell 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b94553f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = Workbook()\n",
    "\n",
    "d = [[\"This dataset is a unification of both old and new CEH deposition headings.\"],\n",
    "     [\"The old and new values are displayed below.\"],\n",
    "     ['easting', 'northing', 'NOx', 'NHx', 'xSOx', 'CaMg', 'year'],\n",
    "     ['easting', 'northing', 'nox_f', 'nhx_f', 'nms_f', 'nm_ca_mg_f', 'year']] \n",
    "\n",
    "ws = wb.new_sheet('README', data=d)\n",
    "\n",
    "ws.set_cell_style(1, 1, Style(font=Font(bold=True)))\n",
    "ws.set_cell_style(1, 1, Style(font=Font(underline=True)))\n",
    "\n",
    "ws.set_cell_style(2, 1, Style(font=Font(bold=True)))\n",
    "ws.set_cell_style(2, 1, Style(font=Font(underline=True)))\n",
    "\n",
    "ws.set_cell_value(3, 7, \"1986-2012\")\n",
    "ws.set_cell_style(3, 7, Style(font=Font(bold=True)))\n",
    "ws.set_cell_style(3, 7, Style(font=Font(underline=True)))\n",
    "\n",
    "ws.set_cell_value(4, 7, \"2012-2019\")\n",
    "ws.set_cell_style(4, 7, Style(font=Font(bold=True)))\n",
    "ws.set_cell_style(4, 7, Style(font=Font(underline=True)))\n",
    "\n",
    "# can set excel cmds via string\n",
    "hlink = ('=HYPERLINK(\"https://catalogue.ceh.ac.uk/documents/fd8151e9-0ee2-4dfa-a254-470c9bb9bc1e\",'\n",
    "         ' \"Link to CEH deposition data\")')\n",
    "\n",
    "ws.set_cell_value(6, 1, \"An example of the original data can be found here\")\n",
    "ws.set_cell_value(7, 1, hlink)\n",
    "\n",
    "wb.save('test.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3e5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eot] *",
   "language": "python",
   "name": "conda-env-eot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
